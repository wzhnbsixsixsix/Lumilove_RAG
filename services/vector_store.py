import chromadb
from chromadb.config import Settings as ChromaSettings
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List, Dict, Any
import uuid
from config import settings

class VectorStoreService:
    def __init__(self):
        # ä½¿ç”¨æ ‡å‡†çš„HuggingFace embeddingæ¨¡å‹
        self.embeddings = HuggingFaceEmbeddings(
            model_name=settings.embedding_model,  # "sentence-transformers/all-MiniLM-L6-v2"
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': False}
        )
        
        print(f"âœ… ä½¿ç”¨HuggingFace embeddingæ¨¡å‹: {settings.embedding_model}")
        
        # åˆå§‹åŒ–ChromaDB
        self.chroma_client = chromadb.PersistentClient(
            path=settings.chroma_persist_directory,
            settings=ChromaSettings(allow_reset=True)
        )
        
        # åˆ›å»ºcollection
        self.collection_name = "chat_history"
        try:
            self.collection = self.chroma_client.get_collection(self.collection_name)
        except:
            self.collection = self.chroma_client.create_collection(self.collection_name)
        
        # åˆå§‹åŒ–LangChainå‘é‡å­˜å‚¨
        self.vector_store = Chroma(
            client=self.chroma_client,
            collection_name=self.collection_name,
            embedding_function=self.embeddings
        )
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        
        print("âœ… å‘é‡å­˜å‚¨æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def add_chat_to_vector_store(self, user_id: str, session_id: str, 
                                conversation_context: List[Dict[str, str]]):
        """å°†èŠå¤©å¯¹è¯æ·»åŠ åˆ°å‘é‡æ•°æ®åº“"""
        documents = []
        metadatas = []
        
        for i, msg in enumerate(conversation_context):
            # åˆ›å»ºæ–‡æ¡£å†…å®¹
            doc_content = f"ç”¨æˆ·: {msg.get('user', '')}\nåŠ©æ‰‹: {msg.get('assistant', '')}"
            
            # åˆ†å‰²é•¿æ–‡æœ¬
            chunks = self.text_splitter.split_text(doc_content)
            
            for chunk in chunks:
                documents.append(chunk)
                metadatas.append({
                    "user_id": user_id,
                    "session_id": session_id,
                    "message_index": i,
                    "type": "conversation",
                    "chunk_id": str(uuid.uuid4())
                })
        
        if documents:
            # ç”Ÿæˆå”¯ä¸€ID
            ids = [str(uuid.uuid4()) for _ in documents]
            
            try:
                # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
                self.vector_store.add_texts(
                    texts=documents,
                    metadatas=metadatas,
                    ids=ids
                )
                print(f"âœ… å·²æ·»åŠ {len(documents)}ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
            except Exception as e:
                print(f"âŒ æ·»åŠ å‘é‡æ•°æ®å¤±è´¥: {e}")
    
    def search_relevant_context(self, query: str, user_id: str, 
                               session_id: str = None, k: int = None) -> List[Dict]:
        """æœç´¢ç›¸å…³çš„ä¸Šä¸‹æ–‡"""
        if k is None:
            k = settings.top_k_results
        
        print(f"ğŸ” å‘é‡æœç´¢å‚æ•°: query='{query}', user_id='{user_id}', session_id='{session_id}', k={k}")
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        if session_id:
            filter_dict = {
                "$and": [
                    {"user_id": user_id},
                    {"session_id": session_id}
                ]
            }
        else:
            filter_dict = {"user_id": user_id}
        
        print(f"ğŸ” ä½¿ç”¨è¿‡æ»¤æ¡ä»¶: {filter_dict}")
        
        try:
            # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
            results = self.vector_store.similarity_search_with_score(
                query=query,
                k=k,
                filter=filter_dict
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            context_results = []
            for doc, score in results:
                # éªŒè¯ç»“æœç¡®å®åŒ¹é…è¿‡æ»¤æ¡ä»¶
                doc_user_id = doc.metadata.get('user_id')
                doc_session_id = doc.metadata.get('session_id')
                
                if str(doc_user_id) == str(user_id) and str(doc_session_id) == str(session_id):
                    context_results.append({
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "similarity_score": score
                    })
                else:
                    print(f"âš ï¸ è­¦å‘Š: è¿‡æ»¤å™¨æœªæ­£ç¡®å·¥ä½œï¼Œæ–‡æ¡£å…ƒæ•°æ®ä¸åŒ¹é…")
                    print(f"   æœŸæœ›: user_id='{user_id}', session_id='{session_id}'")
                    print(f"   å®é™…: user_id='{doc_user_id}', session_id='{doc_session_id}'")
            
            print(f"âœ… æ‰¾åˆ°{len(context_results)}ä¸ªç›¸å…³ä¸Šä¸‹æ–‡")
            return context_results
            
        except Exception as e:
            print(f"âŒ å‘é‡æœç´¢å¤±è´¥: {e}")
            return []
    
    def delete_session_vectors(self, session_id: str):
        """åˆ é™¤ç‰¹å®šä¼šè¯çš„å‘é‡æ•°æ®"""
        try:
            # è·å–è¯¥ä¼šè¯çš„æ‰€æœ‰æ–‡æ¡£ID
            results = self.collection.get(
                where={"session_id": session_id}
            )
            
            if results["ids"]:
                self.collection.delete(ids=results["ids"])
                print(f"âœ… å·²åˆ é™¤ä¼šè¯ {session_id} çš„å‘é‡æ•°æ®")
        except Exception as e:
            print(f"âŒ åˆ é™¤ä¼šè¯å‘é‡å¤±è´¥: {e}")
    
    def get_collection_stats(self):
        """è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            count = self.collection.count()
            return {
                "total_documents": count,
                "collection_name": self.collection_name
            }
        except Exception as e:
            print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "total_documents": 0,
                "collection_name": self.collection_name
            }

# å…¨å±€å‘é‡å­˜å‚¨æœåŠ¡å®ä¾‹
vector_store_service = VectorStoreService()
