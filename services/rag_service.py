from typing import List, Dict, Any, Optional
from .vector_store import vector_store_service
from .chat_service import chat_service
from .openrouter_client import openrouter_client
from config import settings
from .character_service import character_service
import tiktoken  # éœ€è¦å®‰è£…: pip install tiktoken

class RAGService:
    def __init__(self):
        self.vector_store_service = vector_store_service
        self.chat_service = chat_service
        self.openrouter_client = openrouter_client
        # åˆå§‹åŒ–tokenç¼–ç å™¨ï¼ˆä½¿ç”¨GPT-4çš„ç¼–ç å™¨ï¼‰
        self.token_encoder = tiktoken.get_encoding("cl100k_base")
    
    async def generate_response_with_rag(self, user_id: str, session_id: str, 
                                       message: str) -> Dict[str, Any]:
        """ä½¿ç”¨RAG + OpenRouterç”Ÿæˆå›å¤"""
        try:
            # è§£æsession_idå¾—åˆ°character_id
            character_id = self._extract_character_id_from_session(session_id)
            
            # 1. æ£€ç´¢ç›¸å…³çš„å†å²ä¸Šä¸‹æ–‡
            relevant_context = self.vector_store_service.search_relevant_context(
                query=message,
                user_id=user_id,
                session_id=session_id,
                k=settings.top_k_results
            )
            
            # 2. è·å–æœ€è¿‘çš„å¯¹è¯å†å²
            recent_history = await self.chat_service.get_recent_messages(
                session_id=session_id,
                limit=10
            )
            
            # 3. æ„å»ºæç¤ºè¯
            context_text = self._build_context_from_retrieval(relevant_context)
            recent_conversation = self._build_recent_conversation(recent_history)
            
            # 4. æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = self._build_messages(message, context_text, recent_conversation)
            
            # 5. è°ƒç”¨OpenRouterç”Ÿæˆå›å¤
            response = await self.openrouter_client.chat_completion(
                messages=messages,
                max_tokens=2000,
                temperature=0.7
            )
            
            # 6. ä¿å­˜å¯¹è¯åˆ°æ•°æ®åº“ï¼ˆä½¿ç”¨SpringBootçš„è¡¨ç»“æ„ï¼‰
            await self.chat_service.save_message(user_id, character_id, message, response)
            
            # 7. æ›´æ–°å‘é‡æ•°æ®åº“ - ä¿®æ­£æ–¹æ³•å
            conversation_pair = [{"user": message, "assistant": response}]
            self.vector_store_service.add_chat_to_vector_store(
                user_id, session_id, conversation_pair
            )
            
            return {
                "response": response,
                "context_used": [ctx["content"] for ctx in relevant_context],
                "sources": relevant_context,
                "model_info": self.openrouter_client.get_model_info()
            }
            
        except Exception as e:
            raise Exception(f"RAGå“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    def _extract_character_id_from_session(self, session_id: str) -> str:
        """ä»session_idä¸­æå–character_id"""
        # session_idæ ¼å¼: "user_1_character_1"
        try:
            parts = session_id.split('_')
            if len(parts) >= 4 and parts[2] == "character":
                return parts[3]
            return "1"  # é»˜è®¤character_id
        except:
            return "1"
    
    def _build_messages(self, user_message: str, context: str, recent_conversation: str) -> List[Dict[str, str]]:
        """æ„å»ºOpenRouter APIçš„æ¶ˆæ¯æ ¼å¼"""
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½AIåŠ©æ‰‹ï¼Œå…·æœ‰è®°å¿†èƒ½åŠ›ã€‚ä½ å¯ä»¥æ ¹æ®ç”¨æˆ·çš„å†å²å¯¹è¯è®°å½•æ¥æä¾›æ›´ä¸ªæ€§åŒ–å’Œè¿è´¯çš„å›å¤ã€‚

ç›¸å…³å†å²å¯¹è¯ä¸Šä¸‹æ–‡ï¼š
{context}

æœ€è¿‘çš„å¯¹è¯è®°å½•ï¼š
{recent_conversation}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œå¯¹ç”¨æˆ·çš„æ–°æ¶ˆæ¯æä¾›æœ‰å¸®åŠ©çš„å›å¤ã€‚å¦‚æœå†å²å¯¹è¯ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·é€‚å½“å¼•ç”¨ã€‚ä¿æŒå›å¤è‡ªç„¶ã€å‹å¥½å’Œæœ‰ç”¨ã€‚

å½“å‰ä½¿ç”¨çš„AIæ¨¡å‹: {self.openrouter_client.model}"""

        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
    
    def _build_context_from_retrieval(self, relevant_context: List[Dict[str, Any]]) -> str:
        """æ„å»ºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ–‡æœ¬"""
        if not relevant_context:
            return "No relevant historical conversation records found."
        
        context_parts = []
        for i, ctx in enumerate(relevant_context, 1):
            similarity_score = ctx.get("similarity_score", 0)
            content = ctx["content"]
            # æ·»åŠ è°ƒè¯•æ—¥å¿—
            print(f"ğŸ“ ä¸Šä¸‹æ–‡ {i}: {content[:100]}... (ç›¸ä¼¼åº¦: {similarity_score:.3f})")
            context_parts.append(f"Relevant conversation {i} (similarity: {similarity_score:.3f}):\n{content}")
        
        result = "\n\n".join(context_parts)
        print(f"ğŸ” å®Œæ•´ä¸Šä¸‹æ–‡:\n{result}")
        return result
    
    def _build_recent_conversation(self, recent_history: List[Dict]) -> str:
        """æ„å»ºæœ€è¿‘çš„å¯¹è¯å†å²"""
        if not recent_history:
            return "This is the beginning of the conversation."
        
        conversation_parts = []
        for msg in recent_history:
            role = "User" if msg["message_type"] == "user" else "Assistant"
            conversation_parts.append(f"{role}: {msg['content']}")
        
        return "\n".join(conversation_parts)
    
    async def get_available_models(self) -> List[Dict[str, Any]]:
        """è·å–OpenRouterå¯ç”¨æ¨¡å‹"""
        return await self.openrouter_client.get_available_models()
    
    def get_current_model_info(self) -> Dict[str, str]:
        """è·å–å½“å‰æ¨¡å‹ä¿¡æ¯"""
        return self.openrouter_client.get_model_info()

    async def generate_response_with_rag_stream(self, user_id: str, session_id: str, 
                                              message: str):
        """å®Œæ•´çš„RAGå“åº”æµç¨‹ï¼šæç¤ºè¯ + è®°å¿† + å›å¤ + ä¿å­˜"""
        try:
            character_id = self._extract_character_id_from_session(session_id)
            
            # æ­¥éª¤1ï¼šæŸ¥è¯¢è§’è‰²æç¤ºè¯
            print(f"ğŸ“ æ­¥éª¤1: æŸ¥è¯¢è§’è‰²{character_id}çš„æç¤ºè¯...")
            character_prompt = await character_service.get_character_prompt(character_id)
            
            # æ­¥éª¤2ï¼šæ£€ç´¢å†å²è®°å¿†
            print(f"ğŸ§  æ­¥éª¤2: æ£€ç´¢ç”¨æˆ·{user_id}çš„å†å²è®°å¿†...")
            relevant_context = self.vector_store_service.search_relevant_context(
                query=message,
                user_id=user_id,
                session_id=session_id,
                k=settings.top_k_results
            )
            
            # æ­¥éª¤3ï¼šè·å–æœ€è¿‘å¯¹è¯
            print(f"ğŸ’¬ æ­¥éª¤3: è·å–æœ€è¿‘å¯¹è¯å†å²...")
            recent_history = await self.chat_service.get_recent_messages(
                session_id=session_id,
                limit=10
            )
            
            # æ­¥éª¤4ï¼šæ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
            print(f"ğŸ”¨ æ­¥éª¤4: æ„å»ºAIæç¤º...")
            context_text = self._build_context_from_retrieval(relevant_context)
            recent_conversation = self._build_recent_conversation(recent_history)
            
            # æ„å»ºåŒ…å«è§’è‰²è®¾å®šå’Œè®°å¿†çš„å®Œæ•´æç¤º
            messages = self._build_complete_messages(
                user_message=message,
                character_prompt=character_prompt,
                memory_context=context_text,
                recent_conversation=recent_conversation
            )
            
            # æ­¥éª¤5ï¼šç”ŸæˆAIå›å¤ï¼ˆè®°å½•è¾“å‡ºtokenï¼‰
            print(f"ğŸ¤– æ­¥éª¤5: ç”ŸæˆAIå›å¤...")
            complete_response = ""
            output_tokens = 0
            
            async for chunk in self.openrouter_client.chat_completion_stream(
                messages=messages,
                max_tokens=2000,
                temperature=0.7
            ):
                complete_response += chunk
                output_tokens += self._count_tokens(chunk)
                yield {
                    "chunk": chunk,
                    "session_id": session_id,
                    "context_used": [ctx["content"] for ctx in relevant_context],
                    "sources": relevant_context
                }
            
            # æ˜¾ç¤ºè¾“å‡ºtokenç»Ÿè®¡
            total_input_tokens = self._count_tokens(str(messages))
            print("ğŸ“¤ è¾“å‡ºTOKENç»Ÿè®¡:")
            print(f"   è¾“å‡ºtoken: {output_tokens} tokens")
            print(f"   é¢„ä¼°è¾“å‡ºæˆæœ¬: ${output_tokens * 0.000015:.6f} USD")  # å‡è®¾$15/1M tokens
            print(f"   æ€»æˆæœ¬: ${(total_input_tokens * 0.000003 + output_tokens * 0.000015):.6f} USD")
            
            # æ­¥éª¤6ï¼šä¿å­˜åˆ°æ•°æ®åº“å’Œå‘é‡åº“
            print(f"ğŸ’¾ æ­¥éª¤6: ä¿å­˜å¯¹è¯åˆ°æ•°æ®åº“å’Œå‘é‡åº“...")
            
            # ä¿å­˜åˆ°å…³ç³»æ•°æ®åº“
            await self.chat_service.save_message(user_id, character_id, message, complete_response)
            
            # ä¿å­˜åˆ°å‘é‡æ•°æ®åº“ - å¢åŠ è°ƒè¯•ä¿¡æ¯
            conversation_pair = [{"user": message, "assistant": complete_response}]
            print(f"ğŸ”§ å‡†å¤‡ä¿å­˜åˆ°å‘é‡åº“: user_id={user_id}, session_id={session_id}")
            print(f"ğŸ”§ å¯¹è¯å†…å®¹: user='{message}', assistant='{complete_response[:50]}...'")
            
            try:
                self.vector_store_service.add_chat_to_vector_store(
                    user_id, session_id, conversation_pair
                )
                print(f"âœ… å‘é‡æ•°æ®ä¿å­˜æˆåŠŸ")
                
                # ç«‹å³æµ‹è¯•æœç´¢
                test_results = self.vector_store_service.search_relevant_context(
                    query=message,
                    user_id=user_id,
                    session_id=session_id,
                    k=3
                )
                print(f"ğŸ” ä¿å­˜åç«‹å³æœç´¢ç»“æœ: {len(test_results)} æ¡")
                
            except Exception as vec_e:
                print(f"âŒ å‘é‡æ•°æ®ä¿å­˜å¤±è´¥: {vec_e}")
                import traceback
                traceback.print_exc()
            
            print(f"âœ… å®Œæ•´RAGæµç¨‹å®Œæˆï¼")
            
        except Exception as e:
            print(f"âŒ RAGæµç¨‹å¤±è´¥: {e}")
            yield {
                "error": f"RAGæµç¨‹å¤±è´¥: {str(e)}",
                "session_id": session_id
            }

    def _count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        return len(self.token_encoder.encode(text))
    
    def _build_complete_messages(self, user_message: str, character_prompt: str, 
                               memory_context: str, recent_conversation: str) -> List[Dict[str, str]]:
        """æ„å»ºåŒ…å«è§’è‰²è®¾å®šå’Œè®°å¿†çš„å®Œæ•´AIæç¤º"""
        
        system_prompt = f"""You must fulfill the following requirements simultaneously:

ã€Character Settingã€‘
{character_prompt}

Please strictly follow the above character setting for role-playing and maintain character consistency and personality.

ã€Memory Instructionsã€‘
- If the user asks about personal information (name, identity, etc.), you MUST find accurate answers from the following historical memories
- Do not say you don't know or don't remember, use specific information from the historical conversations

ã€Historical Memoryã€‘
{memory_context}

ã€Recent Conversationã€‘
{recent_conversation}

Please maintain your character's personality while accurately using historical memories to answer the user's questions."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºtokenä½¿ç”¨æƒ…å†µ
        self._log_token_usage(character_prompt, memory_context, recent_conversation, 
                              user_message, system_prompt)
        
        return messages
    
    def _log_token_usage(self, character_prompt: str, memory_context: str, 
                        recent_conversation: str, user_message: str, full_system_prompt: str):
        """è¯¦ç»†è®°å½•tokenä½¿ç”¨æƒ…å†µ"""
        
        # åˆ†åˆ«è®¡ç®—å„éƒ¨åˆ†token
        character_tokens = self._count_tokens(character_prompt)
        memory_tokens = self._count_tokens(memory_context)
        recent_tokens = self._count_tokens(recent_conversation)
        user_tokens = self._count_tokens(user_message)
        system_tokens = self._count_tokens(full_system_prompt)
        
        total_input_tokens = system_tokens + user_tokens
        
        print("=" * 60)
        print("ğŸ“Š TOKEN ä½¿ç”¨è¯¦æƒ…")
        print("=" * 60)
        print(f"ğŸ‘¤ ç”¨æˆ·æ¶ˆæ¯: {user_tokens} tokens")
        print(f"ğŸ­ è§’è‰²è®¾å®š: {character_tokens} tokens")
        print(f"ğŸ§  å†å²è®°å¿†: {memory_tokens} tokens")
        print(f"ğŸ’¬ æœ€è¿‘å¯¹è¯: {recent_tokens} tokens")
        print(f"ğŸ“‹ å®Œæ•´ç³»ç»Ÿæç¤º: {system_tokens} tokens")
        print("-" * 60)
        print(f"ğŸ“¥ æ€»è¾“å…¥token: {total_input_tokens} tokens")
        print(f"ğŸ’° é¢„ä¼°è¾“å…¥æˆæœ¬: ${total_input_tokens * 0.000003:.6f} USD")  # å‡è®¾$3/1M tokens
        print("=" * 60)
        
        # æ˜¾ç¤ºå®Œæ•´promptå†…å®¹ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
        if settings.debug:
            print("ğŸ“ å®Œæ•´PROMPTå†…å®¹:")
            print("-" * 40)
            print(full_system_prompt)
            print("-" * 40)
            print(f"ç”¨æˆ·: {user_message}")
            print("=" * 60)

    async def generate_character_response_stream(self, user_id: str, session_id: str, 
                                               message: str, character_prompt: str = ""):
        """ä¸“é—¨ä¸ºè§’è‰²æ‰®æ¼”ä¼˜åŒ–çš„æµå¼å›å¤"""
        try:
            character_id = self._extract_character_id_from_session(session_id)
            
            # 1. æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
            relevant_context = self.vector_store_service.search_relevant_context(
                query=message,
                user_id=user_id,
                session_id=session_id,
                k=settings.top_k_results
            )
            
            # 2. è·å–æœ€è¿‘å¯¹è¯
            recent_history = await self.chat_service.get_recent_messages(
                session_id=session_id,
                limit=10
            )
            
            # 3. æ„å»ºè§’è‰²æ‰®æ¼”æ¶ˆæ¯
            messages = self._build_character_messages(
                message, relevant_context, recent_history, character_prompt
            )
            
            # 4. æµå¼ç”Ÿæˆ
            complete_response = ""
            async for chunk in self.openrouter_client.chat_completion_stream(
                messages=messages,
                max_tokens=2000,
                temperature=0.8  # è§’è‰²æ‰®æ¼”å¯ä»¥æ›´æœ‰åˆ›æ„
            ):
                complete_response += chunk
                yield {"chunk": chunk}
            
            # 5. ä¿å­˜å®Œæ•´å›å¤
            await self.chat_service.save_message(user_id, character_id, message, complete_response)
            
            # 6. æ›´æ–°å‘é‡æ•°æ®åº“
            conversation_pair = [{"user": message, "assistant": complete_response}]
            self.vector_store_service.add_chat_to_vector_store(
                user_id, session_id, conversation_pair
            )
            
        except Exception as e:
            yield {"error": f"è§’è‰²æ‰®æ¼”æµå¼å¤„ç†å¤±è´¥: {str(e)}"}

    def _build_character_messages(self, user_message: str, context: List, 
                                recent_history: List, character_prompt: str) -> List[Dict]:
        """æ„å»ºè§’è‰²æ‰®æ¼”çš„æ¶ˆæ¯"""
        
        context_text = self._build_context_from_retrieval(context)
        recent_conversation = self._build_recent_conversation(recent_history)
        
        system_prompt = f"""è§’è‰²è®¾å®šï¼š
{character_prompt}

å†å²è®°å¿†ä¸Šä¸‹æ–‡ï¼š
{context_text}

æœ€è¿‘çš„å¯¹è¯ï¼š
{recent_conversation}

è¯·ä¸¥æ ¼æŒ‰ç…§è§’è‰²è®¾å®šè¿›è¡Œæ‰®æ¼”ï¼Œç»“åˆå†å²è®°å¿†ï¼Œå¯¹ç”¨æˆ·çš„æ¶ˆæ¯åšå‡ºç¬¦åˆè§’è‰²çš„å›å¤ã€‚ä¿æŒè§’è‰²çš„ä¸€è‡´æ€§å’Œä¸ªæ€§ã€‚"""

        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]

    def _optimize_context_for_tokens(self, relevant_context: List[Dict[str, Any]], 
                                    max_context_tokens: int = 1000) -> str:
        """ä¼˜åŒ–ä¸Šä¸‹æ–‡ä»¥æ§åˆ¶tokenä½¿ç”¨"""
        
        if not relevant_context:
            return "æš‚æ— ç›¸å…³å†å²å¯¹è¯è®°å½•ã€‚"
        
        context_parts = []
        current_tokens = 0
        
        for i, ctx in enumerate(relevant_context, 1):
            similarity_score = ctx.get("similarity_score", 0)
            content = ctx["content"]
            
            # è®¡ç®—è¿™æ¡è®°å½•çš„token
            content_tokens = self._count_tokens(content)
            
            # å¦‚æœåŠ ä¸Šè¿™æ¡è®°å½•ä¼šè¶…è¿‡é™åˆ¶ï¼Œå°±åœæ­¢
            if current_tokens + content_tokens > max_context_tokens:
                print(f"âš ï¸ ä¸Šä¸‹æ–‡tokené™åˆ¶ï¼šåªä½¿ç”¨å‰{i-1}æ¡è®°å½•ï¼ˆ{current_tokens} tokensï¼‰")
                break
            
            context_parts.append(f"å¯¹è¯{i}(ç›¸ä¼¼åº¦{similarity_score:.2f}): {content}")
            current_tokens += content_tokens
        
        result = "\n".join(context_parts)
        print(f"âœ… æœ€ç»ˆä¸Šä¸‹æ–‡: {current_tokens} tokens")
        return result

# å…¨å±€RAGæœåŠ¡å®ä¾‹
rag_service = RAGService()
