from typing import List, Dict, Any, Optional
from .vector_store import vector_store_service
from .chat_service import chat_service
from .openrouter_client import openrouter_client
from config import settings
from .character_service import character_service

class RAGService:
    def __init__(self):
        self.vector_store_service = vector_store_service
        self.chat_service = chat_service
        self.openrouter_client = openrouter_client
    
    async def generate_response_with_rag(self, user_id: str, session_id: str, 
                                       message: str) -> Dict[str, Any]:
        """ä½¿ç”¨RAG + OpenRouterç”Ÿæˆå›å¤"""
        try:
            # è§£æsession_idå¾—åˆ°character_id
            character_id = self._extract_character_id_from_session(session_id)
            
            # 1. æ£€ç´¢ç›¸å…³çš„å†å²ä¸Šä¸‹æ–‡
            relevant_context = self.vector_store_service.search_relevant_context(
                query=message,
                user_id=user_id,
                session_id=session_id,
                k=settings.top_k_results
            )
            
            # 2. è·å–æœ€è¿‘çš„å¯¹è¯å†å²
            recent_history = await self.chat_service.get_recent_messages(
                session_id=session_id,
                limit=10
            )
            
            # 3. æ„å»ºæç¤ºè¯
            context_text = self._build_context_from_retrieval(relevant_context)
            recent_conversation = self._build_recent_conversation(recent_history)
            
            # 4. æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = self._build_messages(message, context_text, recent_conversation)
            
            # 5. è°ƒç”¨OpenRouterç”Ÿæˆå›å¤
            response = await self.openrouter_client.chat_completion(
                messages=messages,
                max_tokens=2000,
                temperature=0.7
            )
            
            # 6. ä¿å­˜å¯¹è¯åˆ°æ•°æ®åº“ï¼ˆä½¿ç”¨SpringBootçš„è¡¨ç»“æ„ï¼‰
            await self.chat_service.save_message(user_id, character_id, message, response)
            
            # 7. æ›´æ–°å‘é‡æ•°æ®åº“ - ä¿®æ­£æ–¹æ³•å
            conversation_pair = [{"user": message, "assistant": response}]
            self.vector_store_service.add_chat_to_vector_store(
                user_id, session_id, conversation_pair
            )
            
            return {
                "response": response,
                "context_used": [ctx["content"] for ctx in relevant_context],
                "sources": relevant_context,
                "model_info": self.openrouter_client.get_model_info()
            }
            
        except Exception as e:
            raise Exception(f"RAGå“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    def _extract_character_id_from_session(self, session_id: str) -> str:
        """ä»session_idä¸­æå–character_id"""
        # session_idæ ¼å¼: "user_1_character_1"
        try:
            parts = session_id.split('_')
            if len(parts) >= 4 and parts[2] == "character":
                return parts[3]
            return "1"  # é»˜è®¤character_id
        except:
            return "1"
    
    def _build_messages(self, user_message: str, context: str, recent_conversation: str) -> List[Dict[str, str]]:
        """æ„å»ºOpenRouter APIçš„æ¶ˆæ¯æ ¼å¼"""
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½AIåŠ©æ‰‹ï¼Œå…·æœ‰è®°å¿†èƒ½åŠ›ã€‚ä½ å¯ä»¥æ ¹æ®ç”¨æˆ·çš„å†å²å¯¹è¯è®°å½•æ¥æä¾›æ›´ä¸ªæ€§åŒ–å’Œè¿è´¯çš„å›å¤ã€‚

ç›¸å…³å†å²å¯¹è¯ä¸Šä¸‹æ–‡ï¼š
{context}

æœ€è¿‘çš„å¯¹è¯è®°å½•ï¼š
{recent_conversation}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œå¯¹ç”¨æˆ·çš„æ–°æ¶ˆæ¯æä¾›æœ‰å¸®åŠ©çš„å›å¤ã€‚å¦‚æœå†å²å¯¹è¯ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·é€‚å½“å¼•ç”¨ã€‚ä¿æŒå›å¤è‡ªç„¶ã€å‹å¥½å’Œæœ‰ç”¨ã€‚

å½“å‰ä½¿ç”¨çš„AIæ¨¡å‹: {self.openrouter_client.model}"""

        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
    
    def _build_context_from_retrieval(self, relevant_context: List[Dict[str, Any]]) -> str:
        """æ„å»ºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ–‡æœ¬"""
        if not relevant_context:
            return "æš‚æ— ç›¸å…³å†å²å¯¹è¯è®°å½•ã€‚"
        
        context_parts = []
        for i, ctx in enumerate(relevant_context, 1):
            similarity_score = ctx.get("similarity_score", 0)
            content = ctx["content"]
            # æ·»åŠ è°ƒè¯•æ—¥å¿—
            print(f"ğŸ“ ä¸Šä¸‹æ–‡ {i}: {content[:100]}... (ç›¸ä¼¼åº¦: {similarity_score:.3f})")
            context_parts.append(f"ç›¸å…³å¯¹è¯ {i} (ç›¸ä¼¼åº¦: {similarity_score:.3f}):\n{content}")
        
        result = "\n\n".join(context_parts)
        print(f"ğŸ” å®Œæ•´ä¸Šä¸‹æ–‡:\n{result}")
        return result
    
    def _build_recent_conversation(self, recent_history: List[Dict]) -> str:
        """æ„å»ºæœ€è¿‘çš„å¯¹è¯å†å²"""
        if not recent_history:
            return "è¿™æ˜¯å¯¹è¯çš„å¼€å§‹ã€‚"
        
        conversation_parts = []
        for msg in recent_history:
            role = "ç”¨æˆ·" if msg["message_type"] == "user" else "åŠ©æ‰‹"
            conversation_parts.append(f"{role}: {msg['content']}")
        
        return "\n".join(conversation_parts)
    
    async def get_available_models(self) -> List[Dict[str, Any]]:
        """è·å–OpenRouterå¯ç”¨æ¨¡å‹"""
        return await self.openrouter_client.get_available_models()
    
    def get_current_model_info(self) -> Dict[str, str]:
        """è·å–å½“å‰æ¨¡å‹ä¿¡æ¯"""
        return self.openrouter_client.get_model_info()

    async def generate_response_with_rag_stream(self, user_id: str, session_id: str, 
                                              message: str):
        """å®Œæ•´çš„RAGå“åº”æµç¨‹ï¼šæç¤ºè¯ + è®°å¿† + å›å¤ + ä¿å­˜"""
        try:
            character_id = self._extract_character_id_from_session(session_id)
            
            # æ­¥éª¤1ï¼šæŸ¥è¯¢è§’è‰²æç¤ºè¯
            print(f"ğŸ“ æ­¥éª¤1: æŸ¥è¯¢è§’è‰²{character_id}çš„æç¤ºè¯...")
            character_prompt = await character_service.get_character_prompt(character_id)
            
            # æ­¥éª¤2ï¼šæ£€ç´¢å†å²è®°å¿†
            print(f"ğŸ§  æ­¥éª¤2: æ£€ç´¢ç”¨æˆ·{user_id}çš„å†å²è®°å¿†...")
            relevant_context = self.vector_store_service.search_relevant_context(
                query=message,
                user_id=user_id,
                session_id=session_id,
                k=settings.top_k_results
            )
            
            # æ­¥éª¤3ï¼šè·å–æœ€è¿‘å¯¹è¯
            print(f"ğŸ’¬ æ­¥éª¤3: è·å–æœ€è¿‘å¯¹è¯å†å²...")
            recent_history = await self.chat_service.get_recent_messages(
                session_id=session_id,
                limit=10
            )
            
            # æ­¥éª¤4ï¼šæ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
            print(f"ğŸ”¨ æ­¥éª¤4: æ„å»ºAIæç¤º...")
            context_text = self._build_context_from_retrieval(relevant_context)
            recent_conversation = self._build_recent_conversation(recent_history)
            
            # æ„å»ºåŒ…å«è§’è‰²è®¾å®šå’Œè®°å¿†çš„å®Œæ•´æç¤º
            messages = self._build_complete_messages(
                user_message=message,
                character_prompt=character_prompt,
                memory_context=context_text,
                recent_conversation=recent_conversation
            )
            
            # æ­¥éª¤5ï¼šç”ŸæˆAIå›å¤
            print(f"ğŸ¤– æ­¥éª¤5: ç”ŸæˆAIå›å¤...")
            complete_response = ""
            async for chunk in self.openrouter_client.chat_completion_stream(
                messages=messages,
                max_tokens=2000,
                temperature=0.7
            ):
                complete_response += chunk
                yield {
                    "chunk": chunk,
                    "session_id": session_id,
                    "context_used": [ctx["content"] for ctx in relevant_context],
                    "sources": relevant_context
                }
            
            # æ­¥éª¤6ï¼šä¿å­˜åˆ°æ•°æ®åº“å’Œå‘é‡åº“
            print(f"ğŸ’¾ æ­¥éª¤6: ä¿å­˜å¯¹è¯åˆ°æ•°æ®åº“å’Œå‘é‡åº“...")
            await self.chat_service.save_message(user_id, character_id, message, complete_response)
            
            # æ­¥éª¤7ï¼šæ·»åŠ åˆ°å‘é‡æ•°æ®åº“ä¾›æœªæ¥æ£€ç´¢
            conversation_pair = [{"user": message, "assistant": complete_response}]
            self.vector_store_service.add_chat_to_vector_store(
                user_id, session_id, conversation_pair
            )
            
            print(f"âœ… å®Œæ•´RAGæµç¨‹å®Œæˆï¼")
            
        except Exception as e:
            print(f"âŒ RAGæµç¨‹å¤±è´¥: {e}")
            yield {
                "error": f"RAGæµç¨‹å¤±è´¥: {str(e)}",
                "session_id": session_id
            }

    def _build_complete_messages(self, user_message: str, character_prompt: str, 
                               memory_context: str, recent_conversation: str) -> List[Dict[str, str]]:
        """æ„å»ºåŒ…å«è§’è‰²è®¾å®šå’Œè®°å¿†çš„å®Œæ•´AIæç¤º"""
        
        # æå–è®°å¿†ä¸­çš„å…³é”®ä¿¡æ¯
        memory_facts = self._extract_memory_facts(memory_context)
        
        system_prompt = f"""{character_prompt}

=== é‡è¦è®°å¿†ä¿¡æ¯ ===
è¯·ç‰¹åˆ«æ³¨æ„ä»¥ä¸‹ä»å†å²å¯¹è¯ä¸­æå–çš„é‡è¦ä¿¡æ¯ï¼š
{memory_facts}

=== è¯¦ç»†å†å²è®°å¿† ===
{memory_context}

=== æœ€è¿‘çš„å¯¹è¯ ===
{recent_conversation}

=== é‡è¦æŒ‡ä»¤ ===
1. é¦–å…ˆæ£€æŸ¥"é‡è¦è®°å¿†ä¿¡æ¯"ä¸­æ˜¯å¦æœ‰ä¸ç”¨æˆ·é—®é¢˜ç›´æ¥ç›¸å…³çš„ç­”æ¡ˆ
2. å¦‚æœç”¨æˆ·é—®åŠå§“åã€èº«ä»½ã€å–œå¥½ç­‰ä¸ªäººä¿¡æ¯ï¼Œå¿…é¡»ä½¿ç”¨å†å²è®°å¿†ä¸­çš„å…·ä½“ä¿¡æ¯å›ç­”
3. åœ¨ä¿æŒè§’è‰²æ€§æ ¼çš„åŒæ—¶ï¼Œå‡†ç¡®ä½¿ç”¨å†å²è®°å¿†ä¸­çš„äº‹å®ä¿¡æ¯
4. å¦‚æœè®°å¿†ä¸­æœ‰æ˜ç¡®ç­”æ¡ˆï¼Œä¸è¦å›é¿æˆ–æ¨¡ç³Šå›ç­”

è¯·åŸºäºè§’è‰²è®¾å®šå’Œå†å²è®°å¿†å›å¤ç”¨æˆ·ã€‚"""

        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]

    def _extract_memory_facts(self, memory_context: str) -> str:
        """ä»è®°å¿†ä¸Šä¸‹æ–‡ä¸­æå–å…³é”®äº‹å®ä¿¡æ¯"""
        facts = []
        
        # ç®€å•çš„å…³é”®ä¿¡æ¯æå–
        if "name is" in memory_context.lower() or "å«" in memory_context:
            # æå–å§“åç›¸å…³ä¿¡æ¯
            lines = memory_context.split('\n')
            for line in lines:
                if ("name is" in line.lower() or "å«" in line) and "ç”¨æˆ·:" in line:
                    facts.append(f"â€¢ ç”¨æˆ·å§“åä¿¡æ¯: {line.strip()}")
        
        if "æˆ‘æ˜¯" in memory_context or "i am" in memory_context.lower():
            lines = memory_context.split('\n')
            for line in lines:
                if ("æˆ‘æ˜¯" in line or "i am" in line.lower()) and "ç”¨æˆ·:" in line:
                    facts.append(f"â€¢ ç”¨æˆ·èº«ä»½ä¿¡æ¯: {line.strip()}")
        
        if facts:
            return "\n".join(facts)
        else:
            return "æš‚æ— æå–åˆ°çš„å…³é”®äº‹å®ä¿¡æ¯"

    async def generate_character_response_stream(self, user_id: str, session_id: str, 
                                               message: str, character_prompt: str = ""):
        """ä¸“é—¨ä¸ºè§’è‰²æ‰®æ¼”ä¼˜åŒ–çš„æµå¼å›å¤"""
        try:
            character_id = self._extract_character_id_from_session(session_id)
            
            # 1. æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
            relevant_context = self.vector_store_service.search_relevant_context(
                query=message,
                user_id=user_id,
                session_id=session_id,
                k=settings.top_k_results
            )
            
            # 2. è·å–æœ€è¿‘å¯¹è¯
            recent_history = await self.chat_service.get_recent_messages(
                session_id=session_id,
                limit=10
            )
            
            # 3. æ„å»ºè§’è‰²æ‰®æ¼”æ¶ˆæ¯
            messages = self._build_character_messages(
                message, relevant_context, recent_history, character_prompt
            )
            
            # 4. æµå¼ç”Ÿæˆ
            complete_response = ""
            async for chunk in self.openrouter_client.chat_completion_stream(
                messages=messages,
                max_tokens=2000,
                temperature=0.8  # è§’è‰²æ‰®æ¼”å¯ä»¥æ›´æœ‰åˆ›æ„
            ):
                complete_response += chunk
                yield {"chunk": chunk}
            
            # 5. ä¿å­˜å®Œæ•´å›å¤
            await self.chat_service.save_message(user_id, character_id, message, complete_response)
            
            # 6. æ›´æ–°å‘é‡æ•°æ®åº“
            conversation_pair = [{"user": message, "assistant": complete_response}]
            self.vector_store_service.add_chat_to_vector_store(
                user_id, session_id, conversation_pair
            )
            
        except Exception as e:
            yield {"error": f"è§’è‰²æ‰®æ¼”æµå¼å¤„ç†å¤±è´¥: {str(e)}"}

    def _build_character_messages(self, user_message: str, context: List, 
                                recent_history: List, character_prompt: str) -> List[Dict]:
        """æ„å»ºè§’è‰²æ‰®æ¼”çš„æ¶ˆæ¯"""
        
        context_text = self._build_context_from_retrieval(context)
        recent_conversation = self._build_recent_conversation(recent_history)
        
        system_prompt = f"""è§’è‰²è®¾å®šï¼š
{character_prompt}

å†å²è®°å¿†ä¸Šä¸‹æ–‡ï¼š
{context_text}

æœ€è¿‘çš„å¯¹è¯ï¼š
{recent_conversation}

è¯·ä¸¥æ ¼æŒ‰ç…§è§’è‰²è®¾å®šè¿›è¡Œæ‰®æ¼”ï¼Œç»“åˆå†å²è®°å¿†ï¼Œå¯¹ç”¨æˆ·çš„æ¶ˆæ¯åšå‡ºç¬¦åˆè§’è‰²çš„å›å¤ã€‚ä¿æŒè§’è‰²çš„ä¸€è‡´æ€§å’Œä¸ªæ€§ã€‚"""

        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]

# å…¨å±€RAGæœåŠ¡å®ä¾‹
rag_service = RAGService()
